Searching
==============================

**Elasticsearch**

In this project, Elasticsearch serves as the vector database responsible for storing and retrieving 
semantically meaningful representations (embeddings) of the documents in our corpus. 
These vector embeddings enable fast and scalable similarity search, which is a core component of 
the Retrieval-Augmented Generation (RAG) pipeline.

**Why Elasticsearch?**

Elasticsearch is traditionally known for keyword-based search using inverted indices. However, 
recent versions (7.3+) support dense vector fields and approximate nearest neighbor (ANN) 
search using methods like HNSW (Hierarchical Navigable Small World graphs), which makes it suitable 
for semantic search in NLP/LLM applications.

**Configuration and Setup**

Embedding Generation:

Documents are first embedded using MPNet (via sentence-transformers). 
Each document is converted into a fixed-size dense vector (typically 768 dimensions).

Index Creation in Elasticsearch:

The index is configured with a dense_vector field to store embeddings.
HNSW is enabled for efficient ANN search.

Following code shows the mapping for the indexing of the vector database.

.. code-block:: python
   :linenos:

    def create_index():
        mapping = {
            "mappings": {
                "properties": {
                    "id": {"type": "keyword"},
                    "page_number": {"type": "keyword"},
                    "title": {"type": "text"},
                    "text": {"type": "text"},
                    "text_embedding": {
                        "type": "dense_vector",
                        "dims": 768,  # <- MPNet output size
                        "index": True,
                        "similarity": "cosine"
                    }
                }
            }
        }

Ingestion:
Each document is ingested along with its embedding. Metadata (e.g., title, source) can also be included to enrich filtering.



**Querying**

The user input is embedded using the same MPNet model. A vector similarity search is performed against the embedding field in Elasticsearch.
The top-k most similar documents are returned, ranked by cosine similarity.

Following code shows the sample quering.

.. code-block:: python
   :linenos:

    dense_query = {
        "size": top_k, # This was set to 5
        "query": {
            "script_score": {
                "query": {"match_all": {}},
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'text_embedding') + 1.0",
                    "params": {"query_vector": query_vec}
                }
            }
        }
    }


**Performance Considerations**

HNSW greatly improves the speed of similarity search compared to brute-force. 
Vector indexing can increase memory usage — tune m, ef_construction, and shard settings appropriately.
Inference time is impacted by both embedding generation and retrieval latency — caching common queries can improve responsiveness.



.. toctree::
   :maxdepth: 6
   :caption: Contents: